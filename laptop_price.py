# -*- coding: utf-8 -*-
"""Laptop_Price.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16aymJpKqN-aJ5lRzgF7W3WwwGretLIiU

# Import Package and Dataset
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Dicoding Machine Learning/Machine Learning Terapan/laptop_price/laptops.csv')

dataset

dataset.info()

dataset.describe()

print(dataset.isna().sum())

"""# Mengatasi Null / Nan Value"""

dataset['GPU'].fillna('None',inplace = True)
dataset.info()

dataset['Storage type'].fillna('SSD', inplace = True)

dataset.isna().sum()

mean_value = dataset['Screen'].mean()
dataset['Screen'] = dataset['Screen'].fillna(mean_value)

dataset[dataset['Screen'].isnull()]

dataset.isna().sum()

"""# Analysis"""

dataset["Brand"].value_counts().plot(kind="bar", xlabel="Brand Laptop", ylabel= "Count", title="Count")

plt.figure(figsize=(8, 6))
sizes = dataset['Status'].value_counts()
plt.pie(sizes, labels=sizes.index, autopct='%1.1f%%')
plt.title('Distribution of Status')
plt.show()

dataset.hist(bins=50, figsize=(20,15))
plt.show()

dataset["CPU"].value_counts()

dataset["Storage type"].value_counts()

sns.catplot(data=dataset,x='RAM',y='Final Price', kind='bar' ,dodge=False, height = 4, aspect = 3,)

plt.scatter(dataset["RAM"], dataset["Storage"])
plt.xlabel("RAM")
plt.ylabel("Storage")

"""# Ubah fitur non-numerik menjadi numerik dengan LabelEncoder"""

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
dataset['Laptop_Encoded'] = label_encoder.fit_transform(dataset['Laptop'])
dataset['Status_Encoded'] = label_encoder.fit_transform(dataset['Status'])
dataset['Brand_Encoded'] = label_encoder.fit_transform(dataset['Brand'])
dataset['Model_Encoded'] = label_encoder.fit_transform(dataset['Model'])
dataset['CPU_Encoded'] = label_encoder.fit_transform(dataset['CPU'])
dataset['Storage type_Encoded'] = label_encoder.fit_transform(dataset['Storage type'])
dataset['GPU_Encoded'] = label_encoder.fit_transform(dataset['GPU'])
dataset['Touch_Encoded'] = label_encoder.fit_transform(dataset['Touch'])

dataset.info()

dataset

df = dataset.drop(columns = ['Laptop', 'Status', 'Brand', 'Model', 'CPU','Storage type','GPU', 'Touch'])

df.info()

df.head()

plt.figure(figsize=(10, 8))
correlation_matrix = df.corr().round(2)

sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix ", size=20)

"""# Split Dataset"""

from sklearn.model_selection import train_test_split

X = df.drop(["Final Price"],axis =1)
y = df["Final Price"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 123)

print(f'Total of sample in whole dataset: {len(X)}')
print(f'Total of sample in train dataset: {len(X_train)}')
print(f'Total of sample in test dataset: {len(X_test)}')

X_train.describe()

from sklearn.preprocessing import StandardScaler


scaler = StandardScaler()

X_train['Storage_scaled'] = scaler.fit_transform(X_train[['Storage']])
X_train['Laptop_Encoded_scaled'] = scaler.fit_transform(X_train[['Laptop_Encoded']])

X_train = X_train.drop(columns=['Storage','Laptop_Encoded'])
X_train.describe()

"""# Build Model"""

build_models = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['KNN', 'RandomForest', 'Boosting'])

"""# Hyper parameter Tuning"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
import numpy as np



# Inisialisasi model dan dictionary hyperparameter
models = {
    'RandomForestRegressor': (RandomForestRegressor(), {
         'n_estimators': [25, 50, 75, 100],
          'max_depth' : [8, 16, 32, 64],
          'random_state': [11, 33, 55, 77],
    }),
    'GradientBoostingRegressor': (GradientBoostingRegressor(), {
         'learning_rate' : [0.1, 0.05, 0.01, 0.05, 0.001],
                'n_estimators': [25, 50, 75, 100],
                'random_state': [11, 33, 55, 77]
    }),
    'KNeighborsRegressor': (KNeighborsRegressor(), {
         'n_neighbors': [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
    })
}

# Proses Randomized Search Cross-Validation untuk masing-masing model
for model_name, (model, param_dist) in models.items():
    print(f"\nTuning hyperparameter untuk {model_name}")
    random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1)
    random_search.fit(X_train, y_train)
    print(f"Kombinasi hyperparameter terbaik untuk {model_name}:", random_search.best_params_)
    print(f"Skor validasi silang terbaik untuk {model_name}:", random_search.best_score_)

"""# K-Nearest Neighbors"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train, y_train)

build_models.loc['train_mse','knn'] = mean_squared_error(y_pred = knn.predict(X_train), y_true=y_train)

"""# Random Forest

Algoritma Bagging
"""

#Impor library yang dibutuhkan
from sklearn.ensemble import RandomForestRegressor

# buat model prediksi
RF = RandomForestRegressor(n_estimators=75, max_depth=32, random_state=33, n_jobs=-1)
RF.fit(X_train, y_train)

build_models.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=RF.predict(X_train), y_true=y_train)

"""Algoritma Boosting"""

from sklearn.ensemble import AdaBoostRegressor

boosting = AdaBoostRegressor(learning_rate=0.1, random_state=77, n_estimators=100)
boosting.fit(X_train, y_train)
build_models.loc['train_mse','Boosting'] = mean_squared_error(y_pred=boosting.predict(X_train), y_true=y_train)

"""# Scalling Data test"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_test['Storage_scaled'] = scaler.fit_transform(X_test[['Storage']])
X_test['Laptop_Encoded_scaled'] = scaler.fit_transform(X_test[['Laptop_Encoded']])

X_test = X_test.drop(columns=['Storage','Laptop_Encoded'])

"""# Predict and using MSE"""

mse = pd.DataFrame(columns=['train', 'test'], index=['KNN','RF','Boosting'])

model_dict = {'KNN': knn, 'RF': RF, 'Boosting': boosting}

# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3

mse

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

prediksi = X_test.iloc[[10]].copy()
y_true = y_test.iloc[[10]]

pred_dict = {'y_true': y_true}

for name, model in model_dict.items():

    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)

hasil_prediksi = pd.DataFrame(pred_dict)

# Menampilkan hasil prediksi
hasil_prediksi